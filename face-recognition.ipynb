{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip uninstall opencv-python-headless -y  \n!pip install opencv-python --upgrade","metadata":{"execution":{"iopub.status.busy":"2022-08-15T12:08:56.931239Z","iopub.execute_input":"2022-08-15T12:08:56.932068Z","iopub.status.idle":"2022-08-15T12:09:15.177231Z","shell.execute_reply.started":"2022-08-15T12:08:56.931965Z","shell.execute_reply":"2022-08-15T12:09:15.175878Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"pip install face-recognition","metadata":{"execution":{"iopub.status.busy":"2022-08-15T12:09:35.758275Z","iopub.execute_input":"2022-08-15T12:09:35.758939Z","iopub.status.idle":"2022-08-15T12:10:09.971662Z","shell.execute_reply.started":"2022-08-15T12:09:35.758899Z","shell.execute_reply":"2022-08-15T12:10:09.970357Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import face_recognition\nfrom PIL import Image, ImageDraw\nimport cv2","metadata":{"execution":{"iopub.status.busy":"2022-08-15T12:10:15.532199Z","iopub.execute_input":"2022-08-15T12:10:15.532577Z","iopub.status.idle":"2022-08-15T12:10:18.473879Z","shell.execute_reply.started":"2022-08-15T12:10:15.532544Z","shell.execute_reply":"2022-08-15T12:10:18.472867Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"\nimage = face_recognition.load_image_file(\"../input/pins-face-recognition/105_classes_pins_dataset/pins_Adriana Lima/Adriana Lima0_0.jpg\")\nface_locations = face_recognition.face_locations(image)\nfor face_location in face_locations:\n\n    # Print the location of each face in this image\n    top, right, bottom, left = face_location\n    print(\"A face is located at pixel location Top: {}, Left: {}, Bottom: {}, Right: {}\".format(top, left, bottom, right))\n\n    # You can access the actual face itself like this:\n    face_image = image[top:bottom, left:right]\n    pil_image = Image.fromarray(face_image)\npil_image","metadata":{"execution":{"iopub.status.busy":"2022-08-14T09:45:35.031682Z","iopub.execute_input":"2022-08-14T09:45:35.032520Z","iopub.status.idle":"2022-08-14T09:45:35.399278Z","shell.execute_reply.started":"2022-08-14T09:45:35.032480Z","shell.execute_reply":"2022-08-14T09:45:35.398286Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"face_landmarks_list = face_recognition.face_landmarks(image)\npil_image = Image.fromarray(image)\nd = ImageDraw.Draw(pil_image)\n\nfor face_landmarks in face_landmarks_list:\n\n    # Print the location of each facial feature in this image\n    for facial_feature in face_landmarks.keys():\n        print(\"The {} in this face has the following points: {}\".format(facial_feature, face_landmarks[facial_feature]))\n\n    # Let's trace out each facial feature in the image with a line!\n    for facial_feature in face_landmarks.keys():\n        d.line(face_landmarks[facial_feature], width=5)\n\n# Show the picture\npil_image","metadata":{"execution":{"iopub.status.busy":"2022-08-14T09:45:58.876658Z","iopub.execute_input":"2022-08-14T09:45:58.877047Z","iopub.status.idle":"2022-08-14T09:45:59.276419Z","shell.execute_reply.started":"2022-08-14T09:45:58.877014Z","shell.execute_reply":"2022-08-14T09:45:59.275385Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"known_image = face_recognition.load_image_file(\"../input/pins-face-recognition/105_classes_pins_dataset/pins_Adriana Lima/Adriana Lima0_0.jpg\")\nunknown_image = face_recognition.load_image_file(\"../input/pins-face-recognition/105_classes_pins_dataset/pins_Adriana Lima/Adriana Lima0_0.jpg\")\ndiff_image=face_recognition.load_image_file(\"../input/pins-face-recognition/105_classes_pins_dataset/pins_Andy Samberg/Andy Samberg0_429.jpg\")\n\nknown_encoding = face_recognition.face_encodings(known_image)[0]\nunknown_encoding = face_recognition.face_encodings(unknown_image)[0]\ndiff_encoding = face_recognition.face_encodings(diff_image)[0]\n\nresults = face_recognition.compare_faces([known_encoding], unknown_encoding)\nresuls_diff=face_recognition.compare_faces([known_encoding], diff_encoding)\nprint(results,resuls_diff)","metadata":{"execution":{"iopub.status.busy":"2022-08-14T09:47:11.366495Z","iopub.execute_input":"2022-08-14T09:47:11.367519Z","iopub.status.idle":"2022-08-14T09:47:11.880553Z","shell.execute_reply.started":"2022-08-14T09:47:11.367478Z","shell.execute_reply":"2022-08-14T09:47:11.879437Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"import face_recognition\n\n\n# This is a demo of running face recognition on a video file and saving the results to a new video file.\n#\n# PLEASE NOTE: This example requires OpenCV (the `cv2` library) to be installed only to read from your webcam.\n# OpenCV is *not* required to use the face_recognition library. It's only required if you want to run this\n# specific demo. If you have trouble installing it, try any of the other demos that don't require it instead.\n\n# Open the input movie file\ninput_movie = cv2.VideoCapture(\"../input/face-recognition-package/examples/hamilton_clip.mp4\")\nlength = int(input_movie.get(cv2.CAP_PROP_FRAME_COUNT))\n\n# Create an output movie file (make sure resolution/frame rate matches input video!)\nfourcc = cv2.VideoWriter_fourcc(*'XVID')\noutput_movie = cv2.VideoWriter('output.avi', fourcc, 29.97, (640, 360))\n\n# Load some sample pictures and learn how to recognize them.\nlmm_image = face_recognition.load_image_file(\"../input/face-recognition-package/examples/lin-manuel-miranda.png\")\nlmm_face_encoding = face_recognition.face_encodings(lmm_image)[0]\n\nal_image = face_recognition.load_image_file(\"../input/face-recognition-package/examples/alex-lacamoire.png\")\nal_face_encoding = face_recognition.face_encodings(al_image)[0]\n\nknown_faces = [\n    lmm_face_encoding,\n    al_face_encoding\n]\n\n# Initialize some variables\nface_locations = []\nface_encodings = []\nface_names = []\nframe_number = 0\n\nwhile True:\n    # Grab a single frame of video\n    ret, frame = input_movie.read()\n    frame_number += 1\n\n    # Quit when the input video file ends\n    if not ret:\n        break\n\n    # Convert the image from BGR color (which OpenCV uses) to RGB color (which face_recognition uses)\n    rgb_frame = frame[:, :, ::-1]\n\n    # Find all the faces and face encodings in the current frame of video\n    face_locations = face_recognition.face_locations(rgb_frame)\n    face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)\n\n    face_names = []\n    for face_encoding in face_encodings:\n        # See if the face is a match for the known face(s)\n        match = face_recognition.compare_faces(known_faces, face_encoding, tolerance=0.50)\n\n        # If you had more than 2 faces, you could make this logic a lot prettier\n        # but I kept it simple for the demo\n        name = None\n        if match[0]:\n            name = \"Lin-Manuel Miranda\"\n        elif match[1]:\n            name = \"Alex Lacamoire\"\n\n        face_names.append(name)\n\n    # Label the results\n    for (top, right, bottom, left), name in zip(face_locations, face_names):\n        if not name:\n            continue\n\n        # Draw a box around the face\n        cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)\n\n        # Draw a label with a name below the face\n        cv2.rectangle(frame, (left, bottom - 25), (right, bottom), (0, 0, 255), cv2.FILLED)\n        font = cv2.FONT_HERSHEY_DUPLEX\n        cv2.putText(frame, name, (left + 6, bottom - 6), font, 0.5, (255, 255, 255), 1)\n\n    # Write the resulting image to the output video file\n    print(\"Writing frame {} / {}\".format(frame_number, length))\n    output_movie.write(frame)\n\n# All done!\ninput_movie.release()\ncv2.destroyAllWindows()","metadata":{"execution":{"iopub.status.busy":"2022-08-14T08:19:37.171885Z","iopub.execute_input":"2022-08-14T08:19:37.172253Z","iopub.status.idle":"2022-08-14T08:25:40.631513Z","shell.execute_reply.started":"2022-08-14T08:19:37.172220Z","shell.execute_reply":"2022-08-14T08:25:40.630400Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Open the input movie file\ninput_movie = cv2.VideoCapture(\"../input/videos1/Joe Biden and Barack Obama at the White House to celebrate the 11th anniversary of the Affordable Care Act on April 5 2022..mp4\")\nlength = int(input_movie.get(cv2.CAP_PROP_FRAME_COUNT))\nwidth  = int(input_movie.get(cv2.CAP_PROP_FRAME_WIDTH))   # float `width`\nheight = int(input_movie.get(cv2.CAP_PROP_FRAME_HEIGHT))  # float `height`\nfbs=int(input_movie.get(cv2.CAP_PROP_FPS))\n\n# Create an output movie file (make sure resolution/frame rate matches input video!)\nfourcc = cv2.VideoWriter_fourcc(*'XVID')\noutput_movie = cv2.VideoWriter('output5.avi', fourcc, fbs, (width, height))\n\n# Load some sample pictures and learn how to recognize them.\nlmm_image = face_recognition.load_image_file(\"../input/face-recognition-package/examples/obama-1080p.jpg\")\nlmm_face_encoding = face_recognition.face_encodings(lmm_image)[0]\n\nal_image = face_recognition.load_image_file(\"../input/face-recognition-package/examples/biden.jpg\")\nal_face_encoding = face_recognition.face_encodings(al_image)[0]\n\nknown_faces = [\n    lmm_face_encoding,\n    al_face_encoding\n]\n\n# Initialize some variables\nface_locations = []\nface_encodings = []\nface_names = []\nframe_number = 0\n\nwhile True:\n    # Grab a single frame of video\n    ret, frame = input_movie.read()\n    frame_number += 1\n\n    # Quit when the input video file ends\n    if not ret:\n        break\n\n    # Convert the image from BGR color (which OpenCV uses) to RGB color (which face_recognition uses)\n    rgb_frame = frame[:, :, ::-1]\n\n    # Find all the faces and face encodings in the current frame of video\n    face_locations = face_recognition.face_locations(rgb_frame)\n    face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)\n\n    face_names = []\n    for face_encoding in face_encodings:\n        # See if the face is a match for the known face(s)\n        match = face_recognition.compare_faces(known_faces, face_encoding, tolerance=1.5)\n\n        # If you had more than 2 faces, you could make this logic a lot prettier\n        # but I kept it simple for the demo\n        name = None\n        if match[0]:\n            name = \"obama\"\n        elif match[1]:\n            name = \"biden\"\n\n        face_names.append(name)\n\n    # Label the results\n    for (top, right, bottom, left), name in zip(face_locations, face_names):\n        if not name:\n            continue\n\n        # Draw a box around the face\n        cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)\n\n        # Draw a label with a name below the face\n        cv2.rectangle(frame, (left, bottom - 25), (right, bottom), (0, 0, 255), cv2.FILLED)\n        font = cv2.FONT_HERSHEY_DUPLEX\n        cv2.putText(frame, name, (left + 6, bottom - 6), font, 0.5, (255, 255, 255), 1)\n\n    # Write the resulting image to the output video file\n    print(\"Writing frame {} / {}\".format(frame_number, length))\n    output_movie.write(frame)\n\n# All done!\ninput_movie.release()\ncv2.destroyAllWindows()","metadata":{"execution":{"iopub.status.busy":"2022-08-14T10:16:35.047701Z","iopub.execute_input":"2022-08-14T10:16:35.048065Z","iopub.status.idle":"2022-08-14T10:20:41.127030Z","shell.execute_reply.started":"2022-08-14T10:16:35.048033Z","shell.execute_reply":"2022-08-14T10:20:41.125910Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"#ob_image_2 = face_recognition.load_image_file(\"../input/face-recognition-package/examples/obama-1080p.jpg\")\nob_image_2 = face_recognition.load_image_file(\"../input/videos1/gettyimages-480661777-612x612.jpg\")\n#ob_image_2=ob_image_2/255\nob_face_encoding_2 = face_recognition.face_encodings(ob_image_2)\n#ob_image_2","metadata":{"execution":{"iopub.status.busy":"2022-08-14T10:56:26.576834Z","iopub.execute_input":"2022-08-14T10:56:26.577265Z","iopub.status.idle":"2022-08-14T10:56:26.736172Z","shell.execute_reply.started":"2022-08-14T10:56:26.577231Z","shell.execute_reply":"2022-08-14T10:56:26.735169Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"# Open the input movie file\ninput_movie = cv2.VideoCapture(\"../input/videos1/MaskUp.mp4\")\nlength = int(input_movie.get(cv2.CAP_PROP_FRAME_COUNT))\nwidth  = int(input_movie.get(cv2.CAP_PROP_FRAME_WIDTH))   # float `width`\nheight = int(input_movie.get(cv2.CAP_PROP_FRAME_HEIGHT))  # float `height`\nfbs=int(input_movie.get(cv2.CAP_PROP_FPS))\n\n# Create an output movie file (make sure resolution/frame rate matches input video!)\nfourcc = cv2.VideoWriter_fourcc(*'XVID')\noutput_movie = cv2.VideoWriter('output12.avi', fourcc, fbs, (width, height))\n\n# Load some sample pictures and learn how to recognize them.\nob_image_1 = face_recognition.load_image_file(\"../input/videos1/Frame162.jpg\")\nob_face_encoding_1 = face_recognition.face_encodings(ob_image_1)[0]\n\nob_image_2 = face_recognition.load_image_file(\"../input/videos1/Frame232.jpg\")\nob_face_encoding_2 = face_recognition.face_encodings(ob_image_2)[0]\n\nbi_image_1 = face_recognition.load_image_file(\"../input/videos1/Frame278.jpg\")\nbi_face_encoding_1 = face_recognition.face_encodings(bi_image_1)[0]\n\nbi_image_2 = face_recognition.load_image_file(\"../input/videos1/Frame34.jpg\")\nbi_face_encoding_2 = face_recognition.face_encodings(bi_image_2)[0]\n\nknown_faces = [\n    ob_face_encoding_1,\n    ob_face_encoding_2,\n    bi_face_encoding_1,\n    bi_face_encoding_2\n]\n\n# Initialize some variables\nface_locations = []\nface_encodings = []\nface_names = []\nframe_number = 0\n\nwhile True:\n    # Grab a single frame of video\n    ret, frame = input_movie.read()\n    frame_number += 1\n\n    # Quit when the input video file ends\n    if not ret:\n        break\n\n    # Convert the image from BGR color (which OpenCV uses) to RGB color (which face_recognition uses)\n    rgb_frame = frame[:, :, ::-1]\n\n    # Find all the faces and face encodings in the current frame of video\n    face_locations = face_recognition.face_locations(rgb_frame)\n    face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)\n\n    face_names = []\n    for face_encoding in face_encodings:\n        # See if the face is a match for the known face(s)\n        match = face_recognition.compare_faces(known_faces, face_encoding, tolerance=0.5)\n\n        # If you had more than 2 faces, you could make this logic a lot prettier\n        # but I kept it simple for the demo\n        name = None\n        if match[0] :\n            name = \"Woman1\"\n        elif match[1]:\n            name = \"Man2\"\n        elif match[2]:\n            name = \"Woman2\"\n        elif match[3]:\n            name = \"Man1\"\n\n        face_names.append(name)\n\n    # Label the results\n    for (top, right, bottom, left), name in zip(face_locations, face_names):\n        if not name:\n            continue\n\n        # Draw a box around the face\n        cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)\n\n        # Draw a label with a name below the face\n        cv2.rectangle(frame, (left, bottom - 25), (right, bottom), (0, 0, 255), cv2.FILLED)\n        font = cv2.FONT_HERSHEY_DUPLEX\n        cv2.putText(frame, name, (left + 6, bottom - 6), font, 0.5, (255, 255, 255), 1)\n\n    # Write the resulting image to the output video file\n    print(\"Writing frame {} / {}\".format(frame_number, length))\n    output_movie.write(frame)\n\n# All done!\ninput_movie.release()\ncv2.destroyAllWindows()","metadata":{"execution":{"iopub.status.busy":"2022-08-14T18:37:36.010042Z","iopub.execute_input":"2022-08-14T18:37:36.010596Z","iopub.status.idle":"2022-08-14T18:39:31.313993Z","shell.execute_reply.started":"2022-08-14T18:37:36.010557Z","shell.execute_reply":"2022-08-14T18:39:31.312841Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Open the input movie file\ninput_movie = cv2.VideoCapture(\"../input/videos1/Wh3 (1).mp4\")\nlength = int(input_movie.get(cv2.CAP_PROP_FRAME_COUNT))\nwidth  = int(input_movie.get(cv2.CAP_PROP_FRAME_WIDTH))   # float `width`\nheight = int(input_movie.get(cv2.CAP_PROP_FRAME_HEIGHT))  # float `height`\nfbs=int(input_movie.get(cv2.CAP_PROP_FPS))\n\n# Create an output movie file (make sure resolution/frame rate matches input video!)\nfourcc = cv2.VideoWriter_fourcc(*'XVID')\noutput_movie = cv2.VideoWriter('Face_Recognition_ouput4.avi', fourcc, fbs, (width, height))\n\n# Load some sample pictures and learn how to recognize them.\nEbram_image = face_recognition.load_image_file(\"../input/videos1/Ebram.jpg\")\nEbram_face_encoding = face_recognition.face_encodings(Ebram_image)[0]\n\nHossam_image = face_recognition.load_image_file(\"../input/videos1/Hossam.jpeg\")\nHossam_face_encoding = face_recognition.face_encodings(Hossam_image)[0]\n\nShehab_image = face_recognition.load_image_file(\"../input/videos1/shehab.jpg\")\nShehab_face_encoding = face_recognition.face_encodings(Shehab_image)[0]\n\nNouran_image = face_recognition.load_image_file(\"../input/videos1/Nouran3.jpeg\")\nNouran_face_encoding = face_recognition.face_encodings(Nouran_image)[0]\n\nShahd_image = face_recognition.load_image_file(\"../input/videos1/Shahd.jpg\")\nShahd_face_encoding = face_recognition.face_encodings(Shahd_image)[0]\n\nknown_faces = [\n    Ebram_face_encoding,\n    Hossam_face_encoding,\n    Shehab_face_encoding,\n    Nouran_face_encoding,\n    Shahd_face_encoding\n]\n\n# Initialize some variables\nface_locations = []\nface_encodings = []\nface_names = []\nframe_number = 0\n\nwhile True:\n    # Grab a single frame of video\n    ret, frame = input_movie.read()\n    frame_number += 1\n\n    # Quit when the input video file ends\n    if not ret:\n        break\n\n    # Convert the image from BGR color (which OpenCV uses) to RGB color (which face_recognition uses)\n    rgb_frame = frame[:, :, ::-1]\n\n    # Find all the faces and face encodings in the current frame of video\n    face_locations = face_recognition.face_locations(rgb_frame)\n    face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)\n\n    face_names = []\n    for face_encoding in face_encodings:\n        # See if the face is a match for the known face(s)\n        match = face_recognition.compare_faces(known_faces, face_encoding, tolerance=0.5)\n\n        # If you had more than 2 faces, you could make this logic a lot prettier\n        # but I kept it simple for the demo\n        name = None\n        if match[0] :\n            name = \"Ebram\"\n        elif match[1]:\n            name = \"Hossam\"\n        elif match[2]:\n            name = \"Shehab\"\n        elif match[3]:\n            name = \"Nouran\"\n        elif match[4]:\n            name = \"Shahd\"\n\n        face_names.append(name)\n\n    # Label the results\n    for (top, right, bottom, left), name in zip(face_locations, face_names):\n        if not name:\n            continue\n\n        # Draw a box around the face\n        cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)\n\n        # Draw a label with a name below the face\n        cv2.rectangle(frame, (left, bottom - 25), (right, bottom), (0, 0, 255), cv2.FILLED)\n        font = cv2.FONT_HERSHEY_DUPLEX\n        cv2.putText(frame, name, (left + 6, bottom - 6), font, 0.5, (255, 255, 255), 1)\n\n    # Write the resulting image to the output video file\n    print(\"Writing frame {} / {}\".format(frame_number, length))\n    output_movie.write(frame)\n\n# All done!\ninput_movie.release()\ncv2.destroyAllWindows()","metadata":{"execution":{"iopub.status.busy":"2022-08-15T12:31:46.298374Z","iopub.execute_input":"2022-08-15T12:31:46.298757Z","iopub.status.idle":"2022-08-15T12:32:56.998599Z","shell.execute_reply.started":"2022-08-15T12:31:46.298718Z","shell.execute_reply":"2022-08-15T12:32:56.997481Z"},"trusted":true},"execution_count":8,"outputs":[]}]}